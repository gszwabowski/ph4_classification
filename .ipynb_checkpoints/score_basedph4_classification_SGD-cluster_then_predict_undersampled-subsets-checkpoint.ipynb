{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing, neighbors\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from typing import Tuple\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster test/train data\n",
    "def get_clusters(X_train: pd.DataFrame, X_test: pd.DataFrame, n_clusters: int) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    applies k-means clustering to training data to find clusters and predicts them for the test set\n",
    "    \"\"\"\n",
    "    clustering = KMeans(n_clusters=n_clusters, random_state=8675309)\n",
    "    clustering.fit(X_train)\n",
    "    # apply the labels\n",
    "    train_labels = clustering.labels_\n",
    "    X_train_clstrs = X_train.copy()\n",
    "    X_train_clstrs['clusters'] = train_labels\n",
    "    \n",
    "    # predict labels on the test set\n",
    "    test_labels = clustering.predict(X_test)\n",
    "    X_test_clstrs = X_test.copy()\n",
    "    X_test_clstrs['clusters'] = test_labels\n",
    "    return X_train_clstrs, X_test_clstrs\n",
    "\n",
    "#scale each feature\n",
    "def scale_features(X_train: pd.DataFrame, X_test: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    applies standard scaler (z-scores) to training data and predicts z-scores for the test set\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    to_scale = [col for col in X_train.columns.values]\n",
    "    scaler.fit(X_train[to_scale])\n",
    "    X_train[to_scale] = scaler.transform(X_train[to_scale])\n",
    "    \n",
    "    # predict z-scores on the test set\n",
    "    X_test[to_scale] = scaler.transform(X_test[to_scale])\n",
    "    \n",
    "    return X_train, X_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictors: ['Hits', 'max_feat', 'min_feat', 'avg_feat', 'max_centr', 'min_centr', 'avg_centr', 'features', 'all_same', 's_score', 'hyd_prop', 'don_prop', 'catdon_prop', 'hydaro_prop', 'aniacc_prop'] \n",
      "\n",
      "x_train Q ph4s: 2711 \n",
      "\n",
      "x_test Q ph4s: 910 \n",
      "\n",
      "0 cluster model\n",
      "\n",
      "Predicted    0    1\n",
      "Actual             \n",
      "0          665  218\n",
      "1           79  769 \n",
      "\n",
      "PPV: 0.78 \n",
      "\n",
      "1 cluster model\n",
      "\n",
      "Predicted   0   1\n",
      "Actual           \n",
      "0          13   5\n",
      "1          24  38 \n",
      "\n",
      "PPV: 0.88\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('..\\..\\data\\_All_Receptors_runs_1_2_3_binary.csv')\n",
    "df.drop(['receptor','Active_Rate','Enrichment', 'GH', 'Actives', 'filename', 'fbase', 'hyd', 'don', 'acc', 'ani', 'cat', 'aro', 'donhyd', 'catdon', 'hydaro', 'aniacc', 'donacc', 'acc_prop', 'ani_prop', 'cat_prop', 'aro_prop', 'donhyd_prop', 'donacc_prop'], 1, inplace=True)\n",
    "df.fillna(-99999)\n",
    "    \n",
    "predictors = list(df.columns)\n",
    "predictors = predictors[:-1]\n",
    "    \n",
    "print('Predictors:', predictors,'\\n')\n",
    "    \n",
    "np.random.seed(42)\n",
    "\n",
    "#split data into quality/not quality sets\n",
    "q_ph4s = df[df['quality'] == 1]\n",
    "nq_ph4s = df[df['quality'] != 1]\n",
    "    \n",
    "#ensure that there is an equal number of nq ph4s\n",
    "nq_ph4s = nq_ph4s.sample(n=1*len(q_ph4s))\n",
    "\n",
    "#merge arrays prior to TTS\n",
    "frames = [q_ph4s, nq_ph4s]\n",
    "df = pd.concat(frames)\n",
    "\n",
    "#x is features, y is classes\n",
    "x = df.drop('quality', 1)\n",
    "y = df.quality\n",
    "    \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=0)\n",
    "    \n",
    "    \n",
    "print(\"x_train Q ph4s:\", y_train.sum(),'\\n')\n",
    "print(\"x_test Q ph4s:\",y_test.sum(),'\\n')\n",
    "    \n",
    "X_train_clstrs, X_test_clstrs = get_clusters(x_train, x_test, 2)\n",
    "    \n",
    "#print(x_train_clstrs)\n",
    "    \n",
    "X_train_scaled, X_test_scaled = scale_features(X_train_clstrs, X_test_clstrs)\n",
    "    \n",
    "#print(X_train_scaled)\n",
    "    \n",
    "# to divide the df by cluster, we need to ensure we use the correct class labels, we'll use pandas to do that\n",
    "train_clusters = X_train_scaled.copy()\n",
    "test_clusters = X_test_scaled.copy()\n",
    "train_clusters['y'] = y_train\n",
    "test_clusters['y'] = y_test\n",
    "\n",
    "#print(y_train)\n",
    "#print(train_clusters)\n",
    "\n",
    "# locate the \"0\" cluster\n",
    "train_0 = train_clusters.loc[train_clusters.clusters < 0] # after scaling, 0 went negtive\n",
    "test_0 = test_clusters.loc[test_clusters.clusters < 0]\n",
    "y_train_0 = train_0.y.values\n",
    "y_test_0 = test_0.y.values\n",
    "# locate the \"1\" cluster\n",
    "train_1 = train_clusters.loc[train_clusters.clusters > 0] # after scaling, 1 dropped slightly\n",
    "test_1 = test_clusters.loc[test_clusters.clusters > 0]\n",
    "y_train_1 = train_1.y.values\n",
    "y_test_1 = test_1.y.values\n",
    "# the base dataset has no \"clusters\" feature\n",
    "X_train_base = X_train_scaled.drop(columns=['clusters'])\n",
    "X_test_base = X_test_scaled.drop(columns=['clusters'])\n",
    "# drop the targets from the training set\n",
    "X_train_0 = train_0.drop(columns=['y'])\n",
    "X_test_0 = test_0.drop(columns=['y'])\n",
    "X_train_1 = train_1.drop(columns=['y'])\n",
    "X_test_1 = test_1.drop(columns=['y'])\n",
    "    \n",
    "#print(X_train_0)\n",
    "#print(len(X_test_1))\n",
    "\n",
    "    \n",
    "#0 cluster LR model\n",
    "sgdc0 = SGDClassifier(loss=\"log\", penalty=\"l1\", max_iter=1000, tol=1e-2, class_weight='balanced')\n",
    "sgdc0.fit(X_train_0, y_train_0)\n",
    "    \n",
    "y_pred = (sgdc0.predict(X_test_0))\n",
    "confmat = confusion_matrix(y_test_0, y_pred, labels=[0,1])\n",
    "#print(confmat)\n",
    "    \n",
    "confmat = confusion_matrix(y_test_0, y_pred, labels=[0,1]).ravel()\n",
    "FP = (confmat[1])\n",
    "TP = (confmat[3])\n",
    "    \n",
    "PPV = (TP / (TP + FP))\n",
    "    \n",
    "cm = pd.crosstab(y_test_0, y_pred, rownames=['Actual'], colnames=['Predicted'], margins=False)\n",
    "    \n",
    "print('0 cluster model\\n')\n",
    "print(cm,'\\n')\n",
    "    \n",
    "print('PPV:', format(PPV, '.2f'),'\\n')\n",
    "    \n",
    "#1 cluster LR model\n",
    "sgdc1 = SGDClassifier(loss=\"log\", penalty=\"l1\", max_iter=1000, tol=1e-2, class_weight='balanced')\n",
    "sgdc1.fit(X_train_1, y_train_1)\n",
    "    \n",
    "y_pred = (sgdc1.predict(X_test_1))\n",
    "confmat = confusion_matrix(y_test_1, y_pred, labels=[0,1])\n",
    "#print(confmat)\n",
    "    \n",
    "confmat = confusion_matrix(y_test_1, y_pred, labels=[0,1]).ravel()\n",
    "FP = (confmat[1])\n",
    "TP = (confmat[3])\n",
    "    \n",
    "PPV = (TP / (TP + FP))\n",
    "    \n",
    "cm = pd.crosstab(y_test_1, y_pred, rownames=['Actual'], colnames=['Predicted'], margins=False)\n",
    "    \n",
    "print('1 cluster model\\n')\n",
    "print(cm,'\\n')\n",
    "\n",
    "print('PPV:', format(PPV, '.2f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scale_features_single(X: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    applies standard scaler (z-scores) to training data and predicts z-scores for the test set\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    to_scale = [col for col in X.columns.values]\n",
    "    scaler.fit(X[to_scale])\n",
    "    X[to_scale] = scaler.transform(X[to_scale])\n",
    "    \n",
    "    return X\n",
    "\n",
    "def classify_ext_data(subset):\n",
    "    #CLassify external data (score based pharmacophore models)\n",
    "    if subset == \"moe\":\n",
    "        ext_df = pd.read_csv('..\\..\\data\\score_based_moefrags_data_binary.csv')\n",
    "    elif subset == \"EF\":\n",
    "        ext_df = pd.read_csv('..\\..\\data\\score_based_efdata_binary.csv')\n",
    "    elif subset == \"GH\":\n",
    "        ext_df = pd.read_csv('..\\..\\data\\score_based_ghdata_binary.csv')\n",
    "    elif subset == \"all\":\n",
    "        ext_df = pd.read_csv('..\\..\\data\\score_based_alldata_binary.csv')\n",
    "    elif subset == \"moe_searches\":\n",
    "        ext_df = pd.read_csv('..\\..\\data\\moefrags_searches.csv')\n",
    "        \n",
    "    ext_df.drop(['Receptor', 'Score Type','Enrichment',  'hyd', 'don', 'acc', 'donhyd', 'catdon', 'hydaro', 'aniacc', 'donacc', 'acc_prop', 'donhyd_prop', 'donacc_prop'], 1, inplace=True)\n",
    "    \n",
    "    #drop extra column from data with searches\n",
    "    if subset == \"moe_searches\":\n",
    "        ext_df.drop(['search_features'], 1, inplace=True)\n",
    "        \n",
    "    ext_df.fillna(-99999)\n",
    "    x = ext_df.drop('quality', 1)\n",
    "    y = ext_df.quality\n",
    "    \n",
    "    predictors = list(ext_df.columns)\n",
    "    predictors = predictors[:-1]\n",
    "    print('Predictors:', predictors,'\\n')\n",
    "    \n",
    "    print(\"score based Q ph4s:\", y.sum(),'\\n')\n",
    "    \n",
    "    clustering = KMeans(n_clusters=2, random_state=8675309)\n",
    "    clustering.fit(x)\n",
    "    \n",
    "    train_labels = clustering.labels_\n",
    "    \n",
    "    X_clstrs = x.copy()\n",
    "    X_clstrs['clusters'] = train_labels\n",
    "    \n",
    "    X_scaled = scale_features_single(X_clstrs)\n",
    "    ext_clusters = X_scaled.copy()\n",
    "    ext_clusters['y'] = y\n",
    "    \n",
    "    # locate the \"0\" cluster\n",
    "    ext_0 = ext_clusters.loc[ext_clusters.clusters < 0] # after scaling, 0 went negtive\n",
    "    y_ext_0 = ext_0.y.values\n",
    "    \n",
    "    # locate the \"1\" cluster\n",
    "    ext_1 = ext_clusters.loc[ext_clusters.clusters > 0] # after scaling, 0 went negtive\n",
    "    y_ext_1 = ext_1.y.values\n",
    "    \n",
    "    # drop the targets from the external set\n",
    "    X_ext_0 = ext_0.drop(columns=['y'])\n",
    "    X_ext_1 = ext_1.drop(columns=['y'])\n",
    "    \n",
    "    #predict based on 0 cluster model\n",
    "    y_pred = (sgdc0.predict(X_ext_0))\n",
    "    confmat = confusion_matrix(y_ext_0, y_pred, labels=[0,1])\n",
    "    #print(confmat)\n",
    "\n",
    "    confmat = confusion_matrix(y_ext_0, y_pred, labels=[0,1]).ravel()\n",
    "    FP = (confmat[1])\n",
    "    TP = (confmat[3])\n",
    "\n",
    "    PPV = (TP / (TP + FP))\n",
    "\n",
    "    cm = pd.crosstab(y_ext_0, y_pred, rownames=['Actual'], colnames=['Predicted'], margins=False)\n",
    "\n",
    "    print('0 cluster model\\n')\n",
    "    print(cm,'\\n')\n",
    "\n",
    "    print('PPV:', format(PPV, '.2f'),'\\n')\n",
    "\n",
    "    #1 cluster LR model\n",
    "    y_pred = (sgdc1.predict(X_ext_1))\n",
    "    confmat = confusion_matrix(y_ext_1, y_pred, labels=[0,1])\n",
    "    #print(confmat)\n",
    "\n",
    "    confmat = confusion_matrix(y_ext_1, y_pred, labels=[0,1]).ravel()\n",
    "    FP = (confmat[1])\n",
    "    TP = (confmat[3])\n",
    "\n",
    "    PPV = (TP / (TP + FP))\n",
    "\n",
    "    cm = pd.crosstab(y_ext_1, y_pred, rownames=['Actual'], colnames=['Predicted'], margins=False)\n",
    "\n",
    "    print('1 cluster model\\n')\n",
    "    print(cm,'\\n')\n",
    "\n",
    "    print('PPV:', format(PPV, '.2f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictors: ['s_score', 'Hits', 'max_feat', 'min_feat', 'avg_feat', 'max_centr', 'min_centr', 'avg_centr', 'features', 'all_same', 'hyd_prop', 'don_prop', 'catdon_prop', 'hydaro_prop', 'aniacc_prop'] \n",
      "\n",
      "score based Q ph4s: 57 \n",
      "\n",
      "0 cluster model\n",
      "\n",
      "Predicted  0  1\n",
      "Actual         \n",
      "0          5  6 \n",
      "\n",
      "PPV: 0.00 \n",
      "\n",
      "1 cluster model\n",
      "\n",
      "Predicted   0   1\n",
      "Actual           \n",
      "0          46  42\n",
      "1          28  29 \n",
      "\n",
      "PPV: 0.41\n"
     ]
    }
   ],
   "source": [
    "classify_ext_data(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictors: ['s_score', 'Hits', 'max_feat', 'min_feat', 'avg_feat', 'max_centr', 'min_centr', 'avg_centr', 'features', 'all_same', 'hyd_prop', 'don_prop', 'catdon_prop', 'hydaro_prop', 'aniacc_prop'] \n",
      "\n",
      "score based Q ph4s: 16 \n",
      "\n",
      "0 cluster model\n",
      "\n",
      "Predicted   0   1\n",
      "Actual           \n",
      "0          17  14\n",
      "1           5  11 \n",
      "\n",
      "PPV: 0.44 \n",
      "\n",
      "1 cluster model\n",
      "\n",
      "Predicted  0  1\n",
      "Actual         \n",
      "0          2  3 \n",
      "\n",
      "PPV: 0.00\n"
     ]
    }
   ],
   "source": [
    "classify_ext_data(\"moe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictors: ['s_score', 'Hits', 'max_feat', 'min_feat', 'avg_feat', 'max_centr', 'min_centr', 'avg_centr', 'features', 'all_same', 'hyd_prop', 'don_prop', 'catdon_prop', 'hydaro_prop', 'aniacc_prop'] \n",
      "\n",
      "score based Q ph4s: 20 \n",
      "\n",
      "0 cluster model\n",
      "\n",
      "Predicted   0   1\n",
      "Actual           \n",
      "0          21   9\n",
      "1          10  10 \n",
      "\n",
      "PPV: 0.53 \n",
      "\n",
      "1 cluster model\n",
      "\n",
      "Predicted  0\n",
      "Actual      \n",
      "0          2 \n",
      "\n",
      "PPV: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-149-dfb3bbde7b4a>:92: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  PPV = (TP / (TP + FP))\n"
     ]
    }
   ],
   "source": [
    "classify_ext_data(\"EF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictors: ['s_score', 'Hits', 'max_feat', 'min_feat', 'avg_feat', 'max_centr', 'min_centr', 'avg_centr', 'features', 'all_same', 'hyd_prop', 'don_prop', 'catdon_prop', 'hydaro_prop', 'aniacc_prop'] \n",
      "\n",
      "score based Q ph4s: 21 \n",
      "\n",
      "0 cluster model\n",
      "\n",
      "Predicted   0   1\n",
      "Actual           \n",
      "0          16  11\n",
      "1           9  12 \n",
      "\n",
      "PPV: 0.52 \n",
      "\n",
      "1 cluster model\n",
      "\n",
      "Predicted  0  1\n",
      "Actual         \n",
      "0          1  3 \n",
      "\n",
      "PPV: 0.00\n"
     ]
    }
   ],
   "source": [
    "classify_ext_data(\"GH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictors: ['Hits', 'max_feat', 'min_feat', 'avg_feat', 'max_centr', 'min_centr', 'avg_centr', 'features', 'all_same', 'hyd_prop', 'catdon_prop', 'aniacc_prop'] \n",
      "\n",
      "7242\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-101-fccc1407e1d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;31m#cluster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[0mclustering\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m999\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[0mclustering\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclustering\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1028\u001b[0m             \u001b[0mFitted\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1029\u001b[0m         \"\"\"\n\u001b[1;32m-> 1030\u001b[1;33m         X = self._validate_data(X, accept_sparse='csr',\n\u001b[0m\u001b[0;32m   1031\u001b[0m                                 \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1032\u001b[0m                                 \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'C'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    418\u001b[0m                     \u001b[1;34mf\"requires y to be passed, but the target y is None.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m                 )\n\u001b[1;32m--> 420\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    421\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 644\u001b[1;33m             _assert_all_finite(array,\n\u001b[0m\u001b[0;32m    645\u001b[0m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0;32m    646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[0;32m     94\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[0;32m     95\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'infinity'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'NaN, infinity'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m     97\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m                     (type_err,\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "#cluster data together \n",
    "df = pd.read_csv('..\\..\\data\\_All_Receptors_runs_1_2_3_binary.csv')\n",
    "df.drop(['s_score','receptor','Active_Rate','Enrichment', 'GH', 'Actives', 'filename', 'fbase', 'hyd', 'don', 'acc', 'ani', 'cat', 'aro', 'donhyd', 'catdon', 'hydaro', 'aniacc', 'donacc','don_prop', 'acc_prop', 'ani_prop', 'cat_prop', 'aro_prop', 'donhyd_prop', 'hydaro_prop', 'donacc_prop'], 1, inplace=True)\n",
    "df.fillna(-99999)\n",
    "\n",
    "df2 = pd.read_csv('..\\..\\data\\score_based_ghdata_binary.csv')\n",
    "df2.drop(['Receptor', 'Score Type','Enrichment',  'hyd', 'don', 'acc', 'donhyd', 'catdon', 'hydaro', 'aniacc', 'donacc', 'don_prop', 'acc_prop', 'donhyd_prop', 'hydaro_prop', 'donacc_prop'], 1, inplace=True)\n",
    "df2.fillna(-99999)    \n",
    "    \n",
    "predictors = list(df.columns)\n",
    "predictors = predictors[:-1]\n",
    "    \n",
    "print('Predictors:', predictors,'\\n')\n",
    "    \n",
    "np.random.seed(42)\n",
    "\n",
    "#split data into quality/not quality sets\n",
    "q_ph4s = df[df['quality'] == 1]\n",
    "nq_ph4s = df[df['quality'] != 1]\n",
    "    \n",
    "#ensure that there is an equal number of nq ph4s\n",
    "nq_ph4s = nq_ph4s.sample(n=1*len(q_ph4s))\n",
    "\n",
    "#merge arrays prior to TTS\n",
    "frames = [q_ph4s, nq_ph4s]\n",
    "df = pd.concat(frames)\n",
    "model_len = len(df)\n",
    "print(model_len)\n",
    "\n",
    "#merge model/external datasets\n",
    "frames = [df, df2]\n",
    "df = pd.concat(frames)\n",
    "\n",
    "#print(df)\n",
    "x = df.drop('quality', 1)\n",
    "y = df.quality\n",
    "\n",
    "#cluster\n",
    "clustering = KMeans(n_clusters=2, random_state=999)\n",
    "clustering.fit(x)\n",
    "train_labels = clustering.labels_\n",
    "print(sum(train_labels))\n",
    "X_clstrs = x.copy()\n",
    "#add clusters column to data\n",
    "X_clstrs['clusters'] = train_labels\n",
    "#add quality labels back to dataset\n",
    "df = pd.concat([X_clstrs, y], axis = 1)\n",
    "#split data back into model set/external set\n",
    "model_df = df.iloc[:model_len,:]\n",
    "ext_df = df.iloc[model_len:,:]\n",
    "\n",
    "#print(model_df)\n",
    "#print(ext_df)\n",
    "#print(len(model_df))\n",
    "#print(len(ext_df))\n",
    "\n",
    "#drop quality labels from modeling df\n",
    "x = model_df.drop('quality', 1)\n",
    "y = model_df.quality\n",
    "\n",
    "#train test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=0)\n",
    "\n",
    "#scale model data\n",
    "X_train_scaled, X_test_scaled = scale_features(x_train, x_test)\n",
    "\n",
    "#pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "#print(X_train_scaled)\n",
    "\n",
    "# to divide the df by cluster, we need to ensure we use the correct class labels, we'll use pandas to do that\n",
    "train_clusters = X_train_scaled.copy()\n",
    "test_clusters = X_test_scaled.copy()\n",
    "train_clusters['y'] = y_train\n",
    "test_clusters['y'] = y_test\n",
    "\n",
    "#print(y_train)\n",
    "#print(train_clusters)\n",
    "\n",
    "# locate the \"0\" cluster\n",
    "train_0 = train_clusters.loc[train_clusters.clusters < 0] # after scaling, 0 went negtive\n",
    "test_0 = test_clusters.loc[test_clusters.clusters < 0]\n",
    "y_train_0 = train_0.y.values\n",
    "y_test_0 = test_0.y.values\n",
    "# locate the \"1\" cluster\n",
    "train_1 = train_clusters.loc[train_clusters.clusters > 0] # after scaling, 1 dropped slightly\n",
    "test_1 = test_clusters.loc[test_clusters.clusters > 0]\n",
    "y_train_1 = train_1.y.values\n",
    "y_test_1 = test_1.y.values\n",
    "# drop the targets from the training set\n",
    "X_train_0 = train_0.drop(columns=['y'])\n",
    "X_test_0 = test_0.drop(columns=['y'])\n",
    "X_train_1 = train_1.drop(columns=['y'])\n",
    "X_test_1 = test_1.drop(columns=['y'])\n",
    "    \n",
    "#print(X_train_0)\n",
    "#print(len(X_test_0))\n",
    "    \n",
    "#0 cluster LR model\n",
    "sgdc0 = SGDClassifier(loss=\"log\", penalty=\"l1\", max_iter=1000, tol=1e-3, class_weight='balanced')\n",
    "sgdc0.fit(X_train_0, y_train_0)\n",
    "    \n",
    "y_pred = (sgdc0.predict(X_test_0))\n",
    "confmat = confusion_matrix(y_test_0, y_pred, labels=[0,1])\n",
    "#print(confmat)\n",
    "    \n",
    "confmat = confusion_matrix(y_test_0, y_pred, labels=[0,1]).ravel()\n",
    "FP = (confmat[1])\n",
    "TP = (confmat[3])\n",
    "    \n",
    "PPV = (TP / (TP + FP))\n",
    "    \n",
    "cm = pd.crosstab(y_test_0, y_pred, rownames=['Actual'], colnames=['Predicted'], margins=False)\n",
    "    \n",
    "print('0 cluster model\\n')\n",
    "print(cm,'\\n')\n",
    "    \n",
    "print('PPV:', format(PPV, '.2f'),'\\n')\n",
    "    \n",
    "#1 cluster LR model\n",
    "sgdc1 = SGDClassifier(loss=\"log\", penalty=\"l1\", max_iter=1000, tol=1e-3, class_weight='balanced')\n",
    "sgdc1.fit(X_train_1, y_train_1)\n",
    "    \n",
    "y_pred = (sgdc1.predict(X_test_1))\n",
    "confmat = confusion_matrix(y_test_1, y_pred, labels=[0,1])\n",
    "#print(confmat)\n",
    "    \n",
    "confmat = confusion_matrix(y_test_1, y_pred, labels=[0,1]).ravel()\n",
    "FP = (confmat[1])\n",
    "TP = (confmat[3])\n",
    "    \n",
    "PPV = (TP / (TP + FP))\n",
    "    \n",
    "cm = pd.crosstab(y_test_1, y_pred, rownames=['Actual'], colnames=['Predicted'], margins=False)\n",
    "    \n",
    "print('1 cluster model\\n')\n",
    "print(cm,'\\n')\n",
    "\n",
    "print('PPV:', format(PPV, '.2f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
