{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing, neighbors\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from typing import Tuple\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import csv\n",
    "\n",
    "randomstate = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster test/train data\n",
    "def get_clusters(X_train: pd.DataFrame, X_test: pd.DataFrame, n_clusters: int) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    applies k-means clustering to training data to find clusters and predicts them for the test set\n",
    "    \"\"\"\n",
    "    clustering = KMeans(n_clusters=n_clusters, random_state=randomstate)\n",
    "    clustering.fit(X_train)\n",
    "    # apply the labels to the training set\n",
    "    train_labels = clustering.labels_\n",
    "    X_train_clstrs = X_train.copy()\n",
    "    X_train_clstrs['clusters'] = train_labels\n",
    "    \n",
    "    #write ext_clusters to csv\n",
    "    X_train_clstrs.to_csv('OLD/X_train_clstrs.csv')\n",
    "    \n",
    "    # predict labels on the test set\n",
    "    test_labels = clustering.predict(X_test)\n",
    "    X_test_clstrs = X_test.copy()\n",
    "    X_test_clstrs['clusters'] = test_labels\n",
    "    return X_train_clstrs, X_test_clstrs\n",
    "\n",
    "#scale each feature\n",
    "def scale_features(X_train: pd.DataFrame, X_test: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    applies standard scaler (z-scores) to training data and predicts z-scores for the test set\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    to_scale = [col for col in X_train.columns.values]\n",
    "    scaler.fit(X_train[to_scale])\n",
    "    X_train[to_scale] = scaler.transform(X_train[to_scale])\n",
    "    \n",
    "    # predict z-scores on the test set\n",
    "    X_test[to_scale] = scaler.transform(X_test[to_scale])\n",
    "    \n",
    "    return X_train, X_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '..\\\\..\\\\data\\\\_All_Receptors_runs_1_2_3_binary.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-a9e18749a466>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'..\\..\\data\\_All_Receptors_runs_1_2_3_binary.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'min_feat'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'receptor'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Active_Rate'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Enrichment'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'GH'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Actives'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'filename'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fbase'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'hyd'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'don'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'acc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ani'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cat'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'aro'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'donhyd'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'catdon'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'hydaro'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'aniacc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'donacc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'acc_prop'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ani_prop'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cat_prop'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'aro_prop'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'donhyd_prop'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'donacc_prop'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m99999\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpredictors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '..\\\\..\\\\data\\\\_All_Receptors_runs_1_2_3_binary.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('..\\..\\data\\_All_Receptors_runs_1_2_3_binary.csv')\n",
    "df.drop(['min_feat','receptor','Active_Rate','Enrichment', 'GH', 'Actives', 'filename', 'fbase', 'hyd', 'don', 'acc', 'ani', 'cat', 'aro', 'donhyd', 'catdon', 'hydaro', 'aniacc', 'donacc', 'acc_prop', 'ani_prop', 'cat_prop', 'aro_prop', 'donhyd_prop', 'donacc_prop'], 1, inplace=True)\n",
    "df.fillna(-99999)\n",
    "    \n",
    "predictors = list(df.columns)\n",
    "predictors = predictors[:-1]\n",
    "    \n",
    "print('Predictors:', predictors,'\\n')\n",
    "    \n",
    "np.random.seed(randomstate)\n",
    "\n",
    "#split data into quality/not quality sets\n",
    "q_ph4s = df[df['quality'] == 1]\n",
    "nq_ph4s = df[df['quality'] != 1]\n",
    "    \n",
    "#ensure that there is an equal number of nq ph4s\n",
    "nq_ph4s = nq_ph4s.sample(n=2*len(q_ph4s))\n",
    "\n",
    "#merge arrays prior to TTS\n",
    "frames = [q_ph4s, nq_ph4s]\n",
    "df = pd.concat(frames)\n",
    "\n",
    "#x is features, y is classes\n",
    "x = df.drop('quality', 1)\n",
    "y = df.quality\n",
    "    \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=randomstate)\n",
    "    \n",
    "    \n",
    "print(\"x_train Q ph4s:\", y_train.sum(),'\\n')\n",
    "print(\"x_test Q ph4s:\",y_test.sum(),'\\n')\n",
    "    \n",
    "X_train_clstrs, X_test_clstrs = get_clusters(x_train, x_test, 4)\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "#print(X_train_clstrs['clusters'].unique())\n",
    "clusters = X_train_clstrs['clusters']\n",
    "\n",
    "X_train_scaled, X_test_scaled = scale_features(X_train_clstrs, X_test_clstrs)\n",
    "    \n",
    "#print(X_train_scaled)\n",
    "    \n",
    "# to divide the df by cluster, we need to ensure we use the correct class labels, we'll use pandas to do that\n",
    "train_clusters = X_train_scaled.copy()\n",
    "test_clusters = X_test_scaled.copy()\n",
    "train_clusters['y'] = y_train\n",
    "test_clusters['y'] = y_test\n",
    "\n",
    "uniq_clusters = train_clusters['clusters'].unique()\n",
    "uniqs = uniq_clusters.tolist()\n",
    "uniqs.sort()\n",
    "print(uniqs)\n",
    "\n",
    "#print(y_train)\n",
    "#print(train_clusters['clusters'])\n",
    "\n",
    "#print(type(clusters))\n",
    "#print(type(train_clusters['clusters']))\n",
    "#frames = [clusters, train_clusters['clusters']]\n",
    "#df = pd.concat(frames, axis=1)\n",
    "#print(type(df))\n",
    "#print(df)\n",
    "#df.to_csv('4clusters.csv')\n",
    "\n",
    "# locate the \"0\" cluster\n",
    "train_0 = train_clusters.loc[train_clusters.clusters <= uniqs[0]] # after scaling, 0 went to -2.187\n",
    "test_0 = test_clusters.loc[test_clusters.clusters <= uniqs[0]]\n",
    "y_train_0 = train_0.y.values\n",
    "y_test_0 = test_0.y.values\n",
    "# locate the \"1\" cluster\n",
    "train_1 = train_clusters.loc[(train_clusters.clusters <= uniqs[1]) & (train_clusters.clusters > uniqs[0])] # after scaling, 1 went to -0.62\n",
    "test_1 = test_clusters.loc[(test_clusters.clusters <= uniqs[1]) & (test_clusters.clusters > uniqs[0])]\n",
    "y_train_1 = train_1.y.values\n",
    "y_test_1 = test_1.y.values\n",
    "\n",
    "# locate the \"2\" cluster\n",
    "train_2 = train_clusters.loc[(train_clusters.clusters <= uniqs[2]) & (train_clusters.clusters > uniqs[1])] # after scaling, 2 went to 0.945\n",
    "test_2 = test_clusters.loc[(test_clusters.clusters <= uniqs[2]) & (test_clusters.clusters > uniqs[1])]\n",
    "y_train_2 = train_2.y.values\n",
    "y_test_2 = test_2.y.values\n",
    "\n",
    "# locate the \"3\" cluster\n",
    "train_3 = train_clusters.loc[train_clusters.clusters >= uniqs[3]] # after scaling, 3 went to 2.51\n",
    "test_3 = test_clusters.loc[test_clusters.clusters >= uniqs[3]]\n",
    "y_train_3 = train_3.y.values\n",
    "y_test_3 = test_3.y.values\n",
    "\n",
    "# drop the targets from the training set\n",
    "X_train_0 = train_0.drop(columns=['y'])\n",
    "X_test_0 = test_0.drop(columns=['y'])\n",
    "X_train_1 = train_1.drop(columns=['y'])\n",
    "X_test_1 = test_1.drop(columns=['y'])\n",
    "X_train_2 = train_2.drop(columns=['y'])\n",
    "X_test_2 = test_2.drop(columns=['y'])\n",
    "X_train_3 = train_3.drop(columns=['y'])\n",
    "X_test_3 = test_3.drop(columns=['y'])\n",
    "\n",
    "print('X_train 1/2/3/4 cluster values\\n')\n",
    "print('-------------------------------\\n')\n",
    "print(X_train_0['clusters'].unique())\n",
    "print(X_train_1['clusters'].unique())\n",
    "print(X_train_2['clusters'].unique())\n",
    "print(X_train_3['clusters'].unique(),'\\n')\n",
    "\n",
    "#print(X_train_0)\n",
    "#print(len(X_test_1))\n",
    "\n",
    "    \n",
    "#0 cluster LR model\n",
    "sgdc0 = SGDClassifier(loss=\"log\", penalty=\"l1\", max_iter=1000, tol=1e-2, class_weight='balanced')\n",
    "sgdc0.fit(X_train_0, y_train_0)\n",
    "    \n",
    "y_pred = (sgdc0.predict(X_test_0))\n",
    "confmat = confusion_matrix(y_test_0, y_pred, labels=[0,1])\n",
    "#print(confmat)\n",
    "    \n",
    "confmat = confusion_matrix(y_test_0, y_pred, labels=[0,1]).ravel()\n",
    "FP = (confmat[1])\n",
    "TP = (confmat[3])\n",
    "    \n",
    "PPV = (TP / (TP + FP))\n",
    "    \n",
    "cm = pd.crosstab(y_test_0, y_pred, rownames=['Actual'], colnames=['Predicted'], margins=False)\n",
    "    \n",
    "print('0 cluster model\\n')\n",
    "print(cm,'\\n')\n",
    "    \n",
    "print('PPV:', format(PPV, '.2f'),'\\n')\n",
    "    \n",
    "#1 cluster LR model\n",
    "sgdc1 = SGDClassifier(loss=\"log\", penalty=\"l1\", max_iter=1000, tol=1e-2, class_weight='balanced')\n",
    "sgdc1.fit(X_train_1, y_train_1)\n",
    "    \n",
    "y_pred = (sgdc1.predict(X_test_1))\n",
    "confmat = confusion_matrix(y_test_1, y_pred, labels=[0,1])\n",
    "    \n",
    "confmat = confusion_matrix(y_test_1, y_pred, labels=[0,1]).ravel()\n",
    "FP = (confmat[1])\n",
    "TP = (confmat[3])\n",
    "    \n",
    "PPV = (TP / (TP + FP))\n",
    "    \n",
    "cm = pd.crosstab(y_test_1, y_pred, rownames=['Actual'], colnames=['Predicted'], margins=False)\n",
    "    \n",
    "print('1 cluster model\\n')\n",
    "print(cm,'\\n')\n",
    "\n",
    "print('PPV:', format(PPV, '.2f'))\n",
    "\n",
    "#print(X_train_0)\n",
    "#print(len(X_train_2))\n",
    "\n",
    "#2 cluster LR model\n",
    "sgdc2 = SGDClassifier(loss=\"log\", penalty=\"l1\", max_iter=1000, tol=1e-2, class_weight='balanced')\n",
    "sgdc2.fit(X_train_2, y_train_2)\n",
    "    \n",
    "y_pred = (sgdc2.predict(X_test_2))\n",
    "confmat = confusion_matrix(y_test_2, y_pred, labels=[0,1])\n",
    "#print(confmat)\n",
    "    \n",
    "confmat = confusion_matrix(y_test_2, y_pred, labels=[0,1]).ravel()\n",
    "FP = (confmat[1])\n",
    "TP = (confmat[3])\n",
    "    \n",
    "PPV = (TP / (TP + FP))\n",
    "    \n",
    "cm = pd.crosstab(y_test_2, y_pred, rownames=['Actual'], colnames=['Predicted'], margins=False)\n",
    "    \n",
    "print('2 cluster model\\n')\n",
    "print(cm,'\\n')\n",
    "    \n",
    "print('PPV:', format(PPV, '.2f'),'\\n')\n",
    "    \n",
    "#3 cluster LR model\n",
    "sgdc3 = SGDClassifier(loss=\"log\", penalty=\"l1\", max_iter=1000, tol=1e-2, class_weight='balanced')\n",
    "sgdc3.fit(X_train_3, y_train_3)\n",
    "    \n",
    "y_pred = (sgdc3.predict(X_test_3))\n",
    "confmat = confusion_matrix(y_test_3, y_pred, labels=[0,1])\n",
    "#print(confmat)\n",
    "    \n",
    "confmat = confusion_matrix(y_test_3, y_pred, labels=[0,1]).ravel()\n",
    "FP = (confmat[1])\n",
    "TP = (confmat[3])\n",
    "    \n",
    "PPV = (TP / (TP + FP))\n",
    "    \n",
    "cm = pd.crosstab(y_test_3, y_pred, rownames=['Actual'], colnames=['Predicted'], margins=False)\n",
    "    \n",
    "print('3 cluster model\\n')\n",
    "print(cm,'\\n')\n",
    "\n",
    "print('PPV:', format(PPV, '.2f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scale_features_single(X: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    applies standard scaler (z-scores) to training data and predicts z-scores for the test set\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    to_scale = [col for col in X.columns.values]\n",
    "    scaler.fit(X[to_scale])\n",
    "    X[to_scale] = scaler.transform(X[to_scale])\n",
    "    \n",
    "    return X\n",
    "\n",
    "def classify_ext_data(subset):\n",
    "    \n",
    "    #CLassify external data (score based pharmacophore models)\n",
    "    if subset == \"moe\":\n",
    "        ext_df = pd.read_csv('..\\..\\data\\score_based_moefrags_data_binary.csv')\n",
    "    elif subset == \"EF\":\n",
    "        ext_df = pd.read_csv('..\\..\\data\\score_based_efdata_binary.csv')\n",
    "    elif subset == \"GH\":\n",
    "        ext_df = pd.read_csv('..\\..\\data\\score_based_ghdata_binary.csv')\n",
    "    elif subset == \"rec_ef\":\n",
    "        ext_df = pd.read_csv('..\\..\\data\\score_based_recefdata_binary.csv')\n",
    "    elif subset == \"rec_gh\":\n",
    "        ext_df = pd.read_csv('..\\..\\data\\score_based_recghdata_binary.csv')\n",
    "    elif subset == \"moe_ef_gh\":\n",
    "        ext_df = pd.read_csv('..\\..\\data\\score_based_moeefgh_data_binary.csv')\n",
    "    elif subset == \"all\":\n",
    "        ext_df = pd.read_csv('..\\..\\data\\score_based_alldata_binary.csv')\n",
    "        \n",
    "    receptors = ext_df.Receptor\n",
    "    hits_actual = ext_df.Hits\n",
    "    ext_df.drop(['min_feat','Receptor', 'Score Type','Enrichment',  'hyd', 'don', 'acc', 'donhyd', 'catdon', 'hydaro', 'aniacc', 'donacc', 'acc_prop', 'donhyd_prop', 'donacc_prop'], 1, inplace=True)\n",
    "    \n",
    "    #drop extra column from data with searches\n",
    "    if subset == \"moe_searches\":\n",
    "        ext_df.drop(['search_features'], 1, inplace=True)\n",
    "        \n",
    "    ext_df.fillna(-99999)\n",
    "    x = ext_df.drop('quality', 1)\n",
    "    y = ext_df.quality\n",
    "    \n",
    "    predictors = list(ext_df.columns)\n",
    "    predictors = predictors[:-1]\n",
    "    print('Predictors:', predictors,'\\n')\n",
    "    \n",
    "    print(\"score based Q ph4s:\", y.sum(),'\\n')\n",
    "    \n",
    "    #cluster training data\n",
    "    clustering = KMeans(n_clusters=4, random_state=randomstate)\n",
    "    clustering.fit(x_train)\n",
    "    # apply the labels to the training set\n",
    "    train_labels = clustering.labels_\n",
    "    X_train_clstrs = x_train.copy()\n",
    "    X_train_clstrs['clusters'] = train_labels\n",
    "    \n",
    "    # predict labels on the external set\n",
    "    ext_labels = clustering.predict(x)\n",
    "    X_clstrs = x.copy()\n",
    "    X_clstrs['clusters'] = ext_labels\n",
    "    \n",
    "    X_scaled = scale_features_single(X_clstrs)\n",
    "    ext_clusters = X_scaled.copy()\n",
    "    ext_clusters['y'] = y\n",
    "    \n",
    "    #add receptors and hits_actual columns back prior to 0/1/2/3 split\n",
    "    ext_clusters['Receptor'] = receptors\n",
    "    ext_clusters['hits_actual'] = hits_actual\n",
    "    \n",
    "    #write ext_clusters to csv\n",
    "    X_train_clstrs.to_csv('X_train_clstrs2.csv')\n",
    "    \n",
    "    # locate the \"0\" cluster\n",
    "    ext_0 = ext_clusters.loc[ext_clusters.clusters <= uniqs[0]] # after scaling, 0 went negtive\n",
    "    y_ext_0 = ext_0.y.values\n",
    "    ext_0_receptors = ext_0.Receptor\n",
    "    ext_0_hits_actual = ext_0.hits_actual\n",
    "    \n",
    "    # locate the \"1\" cluster\n",
    "    ext_1 = ext_clusters.loc[(ext_clusters.clusters <= uniqs[1]) & (ext_clusters.clusters > uniqs[0])] # after scaling, 0 went negtive\n",
    "    y_ext_1 = ext_1.y.values\n",
    "    ext_1_receptors = ext_1.Receptor\n",
    "    ext_1_hits_actual = ext_1.hits_actual\n",
    "    \n",
    "    # locate the \"2\" cluster\n",
    "    ext_2 = ext_clusters.loc[(ext_clusters.clusters <= uniqs[2]) & (ext_clusters.clusters > uniqs[1])] # after scaling, 0 went negtive\n",
    "    y_ext_2 = ext_2.y.values\n",
    "    ext_2_receptors = ext_2.Receptor\n",
    "    ext_2_hits_actual = ext_2.hits_actual\n",
    "    \n",
    "    # locate the \"3\" cluster\n",
    "    ext_3 = ext_clusters.loc[ext_clusters.clusters > uniqs[3] ] # after scaling, 0 went negtive\n",
    "    y_ext_3 = ext_3.y.values\n",
    "    ext_3_receptors = ext_3.Receptor\n",
    "    ext_3_hits_actual = ext_3.hits_actual\n",
    "\n",
    "    # drop the targets from each external set\n",
    "    X_ext_0 = ext_0.drop(columns=['y', 'Receptor', 'hits_actual'])\n",
    "    X_ext_1 = ext_1.drop(columns=['y', 'Receptor', 'hits_actual'])\n",
    "    X_ext_2 = ext_2.drop(columns=['y', 'Receptor', 'hits_actual'])\n",
    "    X_ext_3 = ext_3.drop(columns=['y', 'Receptor', 'hits_actual'])\n",
    "    \n",
    "    # drop receptor column from each external set\n",
    "    \n",
    "    #print(len(X_ext_0))\n",
    "    #print(len(X_ext_1))\n",
    "    #print(len(X_ext_2))\n",
    "    #print(len(X_ext_3))\n",
    "    \n",
    "    #predict based on 0 cluster model\n",
    "    print('0 cluster model\\n')\n",
    "    if len(X_ext_0) == 0:\n",
    "        print('No cluster 0 data.\\n')\n",
    "    else:\n",
    "        y_pred = (sgdc0.predict(X_ext_0))\n",
    "        confmat = confusion_matrix(y_ext_0, y_pred, labels=[0,1])\n",
    "        #print(confmat)\n",
    "\n",
    "        confmat = confusion_matrix(y_ext_0, y_pred, labels=[0,1]).ravel()\n",
    "        FP = (confmat[1])\n",
    "        TP = (confmat[3])\n",
    "\n",
    "        PPV = (TP / (TP + FP))\n",
    "\n",
    "        cm = pd.crosstab(y_ext_0, y_pred, rownames=['Actual'], colnames=['Predicted'], margins=False)\n",
    "\n",
    "    \n",
    "        print(cm,'\\n')\n",
    "\n",
    "        print('PPV:', format(PPV, '.2f'),'\\n')\n",
    "        \n",
    "        X_ext_0['Receptor'] = ext_0_receptors\n",
    "        X_ext_0['hits_actual'] = ext_0_hits_actual\n",
    "        X_ext_0['quality'] = y_ext_0\n",
    "        X_ext_0['quality_pred'] = y_pred\n",
    "        X_ext_0.to_csv('results/'+subset+'/0cluster_results.csv')\n",
    "\n",
    "    print('1 cluster model\\n')\n",
    "    if len(X_ext_1) == 0:\n",
    "        print('No cluster 1 data.\\n')\n",
    "    else:\n",
    "        y_pred = (sgdc1.predict(X_ext_1))\n",
    "        confmat = confusion_matrix(y_ext_1, y_pred, labels=[0,1])\n",
    "        #print(confmat)\n",
    "\n",
    "        confmat = confusion_matrix(y_ext_1, y_pred, labels=[0,1]).ravel()\n",
    "        FP = (confmat[1])\n",
    "        TP = (confmat[3])\n",
    "\n",
    "        PPV = (TP / (TP + FP))\n",
    "\n",
    "        cm = pd.crosstab(y_ext_1, y_pred, rownames=['Actual'], colnames=['Predicted'], margins=False)\n",
    "\n",
    "    \n",
    "        print(cm,'\\n')\n",
    "\n",
    "        print('PPV:', format(PPV, '.2f'),'\\n')\n",
    "        \n",
    "        X_ext_1['Receptor'] = ext_1_receptors\n",
    "        X_ext_1['hits_actual'] = ext_1_hits_actual\n",
    "        X_ext_1['quality'] = y_ext_1\n",
    "        X_ext_1['quality_pred'] = y_pred\n",
    "        X_ext_1.to_csv('results/'+subset+'/1cluster_results.csv')\n",
    "        \n",
    "    print('2 cluster model\\n')\n",
    "    if len(X_ext_2) == 0:\n",
    "        print('No cluster 2 data.\\n')\n",
    "    else:\n",
    "        y_pred = (sgdc2.predict(X_ext_2))\n",
    "        confmat = confusion_matrix(y_ext_2, y_pred, labels=[0,1])\n",
    "        #print(confmat)\n",
    "\n",
    "        confmat = confusion_matrix(y_ext_2, y_pred, labels=[0,1]).ravel()\n",
    "        FP = (confmat[1])\n",
    "        TP = (confmat[3])\n",
    "\n",
    "        PPV = (TP / (TP + FP))\n",
    "\n",
    "        cm = pd.crosstab(y_ext_2, y_pred, rownames=['Actual'], colnames=['Predicted'], margins=False)\n",
    "\n",
    "    \n",
    "        print(cm,'\\n')\n",
    "\n",
    "        print('PPV:', format(PPV, '.2f'),'\\n')\n",
    "        \n",
    "        X_ext_2['Receptor'] = ext_2_receptors\n",
    "        X_ext_2['hits_actual'] = ext_2_hits_actual\n",
    "        X_ext_2['quality'] = y_ext_2\n",
    "        X_ext_2['quality_pred'] = y_pred\n",
    "        X_ext_2.to_csv('results/'+subset+'/2cluster_results.csv')\n",
    "        \n",
    "    print('3 cluster model\\n')\n",
    "    if len(X_ext_3) == 0:\n",
    "        print('No cluster 3 data.\\n')\n",
    "    else:\n",
    "        y_pred = (sgdc3.predict(X_ext_3))\n",
    "        confmat = confusion_matrix(y_ext_3, y_pred, labels=[0,1])\n",
    "        #print(confmat)\n",
    "\n",
    "        confmat = confusion_matrix(y_ext_3, y_pred, labels=[0,1]).ravel()\n",
    "        FP = (confmat[1])\n",
    "        TP = (confmat[3])\n",
    "\n",
    "        PPV = (TP / (TP + FP))\n",
    "\n",
    "        cm = pd.crosstab(y_ext_3, y_pred, rownames=['Actual'], colnames=['Predicted'], margins=False)\n",
    "\n",
    "    \n",
    "        print(cm,'\\n')\n",
    "\n",
    "        print('PPV:', format(PPV, '.2f'),'\\n')\n",
    "        \n",
    "        X_ext_3['Receptor'] = ext_3_receptors\n",
    "        X_ext_3['hits_actual'] = ext_3_hits_actual\n",
    "        X_ext_3['quality'] = y_ext_3\n",
    "        X_ext_3['quality_pred'] = y_pred\n",
    "        X_ext_3.to_csv('results/'+subset+'/3cluster_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '..\\\\..\\\\data\\\\score_based_alldata_binary.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-d64d60f8bec4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclassify_ext_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"all\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-9f599ab03c71>\u001b[0m in \u001b[0;36mclassify_ext_data\u001b[1;34m(subset)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mext_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'..\\..\\data\\score_based_moeefgh_data_binary.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0msubset\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"all\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mext_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'..\\..\\data\\score_based_alldata_binary.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mreceptors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mext_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReceptor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '..\\\\..\\\\data\\\\score_based_alldata_binary.csv'"
     ]
    }
   ],
   "source": [
    "classify_ext_data(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictors: ['s_score', 'Hits', 'max_feat', 'avg_feat', 'max_centr', 'min_centr', 'avg_centr', 'features', 'all_same', 'hyd_prop', 'don_prop', 'catdon_prop', 'hydaro_prop', 'aniacc_prop'] \n",
      "\n",
      "score based Q ph4s: 16 \n",
      "\n",
      "0 cluster model\n",
      "\n",
      "No cluster 0 data.\n",
      "\n",
      "1 cluster model\n",
      "\n",
      "Predicted  0  1\n",
      "Actual         \n",
      "0          3  1\n",
      "1          6  6 \n",
      "\n",
      "PPV: 0.86 \n",
      "\n",
      "2 cluster model\n",
      "\n",
      "Predicted   0   1\n",
      "Actual           \n",
      "0          10  17\n",
      "1           2   2 \n",
      "\n",
      "PPV: 0.11 \n",
      "\n",
      "3 cluster model\n",
      "\n",
      "No cluster 3 data.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Greg\\anaconda3\\lib\\site-packages\\sklearn\\base.py:488: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names must be in the same order as they were in fit.\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Greg\\anaconda3\\lib\\site-packages\\sklearn\\base.py:488: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names must be in the same order as they were in fit.\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Greg\\anaconda3\\lib\\site-packages\\sklearn\\base.py:488: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names must be in the same order as they were in fit.\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "classify_ext_data(\"moe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictors: ['s_score', 'Hits', 'max_feat', 'avg_feat', 'max_centr', 'min_centr', 'avg_centr', 'features', 'all_same', 'hyd_prop', 'don_prop', 'catdon_prop', 'hydaro_prop', 'aniacc_prop'] \n",
      "\n",
      "score based Q ph4s: 20 \n",
      "\n",
      "0 cluster model\n",
      "\n",
      "No cluster 0 data.\n",
      "\n",
      "1 cluster model\n",
      "\n",
      "Predicted  0  1\n",
      "Actual         \n",
      "0          2  0\n",
      "1          5  4 \n",
      "\n",
      "PPV: 1.00 \n",
      "\n",
      "2 cluster model\n",
      "\n",
      "Predicted   0   1\n",
      "Actual           \n",
      "0          16  12\n",
      "1           4   7 \n",
      "\n",
      "PPV: 0.37 \n",
      "\n",
      "3 cluster model\n",
      "\n",
      "No cluster 3 data.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Greg\\anaconda3\\lib\\site-packages\\sklearn\\base.py:488: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names must be in the same order as they were in fit.\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Greg\\anaconda3\\lib\\site-packages\\sklearn\\base.py:488: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names must be in the same order as they were in fit.\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Greg\\anaconda3\\lib\\site-packages\\sklearn\\base.py:488: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names must be in the same order as they were in fit.\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "classify_ext_data(\"EF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictors: ['s_score', 'Hits', 'max_feat', 'avg_feat', 'max_centr', 'min_centr', 'avg_centr', 'features', 'all_same', 'hyd_prop', 'don_prop', 'catdon_prop', 'hydaro_prop', 'aniacc_prop'] \n",
      "\n",
      "score based Q ph4s: 21 \n",
      "\n",
      "0 cluster model\n",
      "\n",
      "No cluster 0 data.\n",
      "\n",
      "1 cluster model\n",
      "\n",
      "Predicted  0  1\n",
      "Actual         \n",
      "0          3  1\n",
      "1          9  6 \n",
      "\n",
      "PPV: 0.86 \n",
      "\n",
      "2 cluster model\n",
      "\n",
      "Predicted  0   1\n",
      "Actual          \n",
      "0          8  15\n",
      "1          3   3 \n",
      "\n",
      "PPV: 0.17 \n",
      "\n",
      "3 cluster model\n",
      "\n",
      "No cluster 3 data.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Greg\\anaconda3\\lib\\site-packages\\sklearn\\base.py:488: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names must be in the same order as they were in fit.\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Greg\\anaconda3\\lib\\site-packages\\sklearn\\base.py:488: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names must be in the same order as they were in fit.\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Greg\\anaconda3\\lib\\site-packages\\sklearn\\base.py:488: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names must be in the same order as they were in fit.\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "classify_ext_data(\"GH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictors: ['s_score', 'Hits', 'max_feat', 'avg_feat', 'max_centr', 'min_centr', 'avg_centr', 'features', 'all_same', 'hyd_prop', 'don_prop', 'catdon_prop', 'hydaro_prop', 'aniacc_prop'] \n",
      "\n",
      "score based Q ph4s: 13 \n",
      "\n",
      "0 cluster model\n",
      "\n",
      "No cluster 0 data.\n",
      "\n",
      "1 cluster model\n",
      "\n",
      "Predicted  0  1\n",
      "Actual         \n",
      "0          1  0\n",
      "1          4  4 \n",
      "\n",
      "PPV: 1.00 \n",
      "\n",
      "2 cluster model\n",
      "\n",
      "Predicted   0   1\n",
      "Actual           \n",
      "0          18  18\n",
      "1           1   4 \n",
      "\n",
      "PPV: 0.18 \n",
      "\n",
      "3 cluster model\n",
      "\n",
      "No cluster 3 data.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Greg\\anaconda3\\lib\\site-packages\\sklearn\\base.py:488: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names must be in the same order as they were in fit.\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Greg\\anaconda3\\lib\\site-packages\\sklearn\\base.py:488: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names must be in the same order as they were in fit.\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Greg\\anaconda3\\lib\\site-packages\\sklearn\\base.py:488: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names must be in the same order as they were in fit.\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "classify_ext_data(\"rec_ef\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictors: ['s_score', 'Hits', 'max_feat', 'avg_feat', 'max_centr', 'min_centr', 'avg_centr', 'features', 'all_same', 'hyd_prop', 'don_prop', 'catdon_prop', 'hydaro_prop', 'aniacc_prop'] \n",
      "\n",
      "score based Q ph4s: 15 \n",
      "\n",
      "0 cluster model\n",
      "\n",
      "No cluster 0 data.\n",
      "\n",
      "1 cluster model\n",
      "\n",
      "Predicted  0  1\n",
      "Actual         \n",
      "0          1  2\n",
      "1          6  2 \n",
      "\n",
      "PPV: 0.50 \n",
      "\n",
      "2 cluster model\n",
      "\n",
      "Predicted  0   1\n",
      "Actual          \n",
      "0          9  22\n",
      "1          6   1 \n",
      "\n",
      "PPV: 0.04 \n",
      "\n",
      "3 cluster model\n",
      "\n",
      "No cluster 3 data.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Greg\\anaconda3\\lib\\site-packages\\sklearn\\base.py:488: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names must be in the same order as they were in fit.\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Greg\\anaconda3\\lib\\site-packages\\sklearn\\base.py:488: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names must be in the same order as they were in fit.\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Greg\\anaconda3\\lib\\site-packages\\sklearn\\base.py:488: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names must be in the same order as they were in fit.\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "classify_ext_data(\"rec_gh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_sgdc3']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save models\n",
    "#import joblib\n",
    "#from joblib import dump, load\n",
    "\n",
    "#joblib.dump(sgdc0 , 'model_sgdc0')\n",
    "#joblib.dump(sgdc0 , 'model_sgdc1')\n",
    "#joblib.dump(sgdc0 , 'model_sgdc2')\n",
    "#joblib.dump(sgdc0 , 'model_sgdc3')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
