{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing, neighbors\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from typing import Tuple\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import pandas as pd\n",
    "import imblearn\n",
    "from imblearn.over_sampling import RandomOverSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster test/train data\n",
    "def get_clusters(X_train: pd.DataFrame, X_test: pd.DataFrame, n_clusters: int) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    applies k-means clustering to training data to find clusters and predicts them for the test set\n",
    "    \"\"\"\n",
    "    clustering = KMeans(n_clusters=n_clusters, random_state=8675309)\n",
    "    clustering.fit(X_train)\n",
    "    # apply the labels\n",
    "    train_labels = clustering.labels_\n",
    "    X_train_clstrs = X_train.copy()\n",
    "    X_train_clstrs['clusters'] = train_labels\n",
    "    \n",
    "    # predict labels on the test set\n",
    "    test_labels = clustering.predict(X_test)\n",
    "    X_test_clstrs = X_test.copy()\n",
    "    X_test_clstrs['clusters'] = test_labels\n",
    "    return X_train_clstrs, X_test_clstrs\n",
    "\n",
    "#scale each feature\n",
    "def scale_features(X_train: pd.DataFrame, X_test: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    applies standard scaler (z-scores) to training data and predicts z-scores for the test set\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    to_scale = [col for col in X_train.columns.values]\n",
    "    scaler.fit(X_train[to_scale])\n",
    "    X_train[to_scale] = scaler.transform(X_train[to_scale])\n",
    "    \n",
    "    # predict z-scores on the test set\n",
    "    X_test[to_scale] = scaler.transform(X_test[to_scale])\n",
    "    \n",
    "    return X_train, X_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictors: ['Hits', 'max_feat', 'min_feat', 'avg_feat', 'max_centr', 'min_centr', 'avg_centr', 'features', 'all_same', 's_score', 'hyd_prop', 'catdon_prop', 'aniacc_prop'] \n",
      "\n",
      "x_train Q ph4s: 84412 \n",
      "\n",
      "x_test Q ph4s: 28186 \n",
      "\n",
      "0 cluster model\n",
      "\n",
      "Predicted      0      1\n",
      "Actual                 \n",
      "0          21721   5794\n",
      "1           2377  23804 \n",
      "\n",
      "PPV: 0.80 \n",
      "\n",
      "1 cluster model\n",
      "\n",
      "Predicted   0     1\n",
      "Actual             \n",
      "0          99   499\n",
      "1          74  1931 \n",
      "\n",
      "PPV: 0.79\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('..\\data\\_All_Receptors_runs_1_2_3_binary.csv')\n",
    "df.drop(['receptor','Active_Rate','Enrichment', 'GH', 'Actives', 'filename', 'fbase', 'hyd', 'don', 'acc', 'ani', 'cat', 'aro', 'donhyd', 'catdon', 'hydaro', 'aniacc', 'donacc','don_prop', 'acc_prop', 'ani_prop', 'cat_prop', 'aro_prop', 'donhyd_prop', 'hydaro_prop', 'donacc_prop'], 1, inplace=True)\n",
    "df.fillna(-99999)\n",
    "\n",
    "# define oversampling strategy\n",
    "oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "\n",
    "predictors = list(df.columns)\n",
    "predictors = predictors[:-1]\n",
    "\n",
    "print('Predictors:', predictors,'\\n')\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "#x is features, y is classes\n",
    "x = df.drop('quality', 1)\n",
    "y = df.quality\n",
    "\n",
    "#create oversampled dataset\n",
    "x_over, y_over = oversample.fit_resample(x, y)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_over, y_over, test_size=0.25, random_state=0)\n",
    "\n",
    "\n",
    "print(\"x_train Q ph4s:\", y_train.sum(),'\\n')\n",
    "print(\"x_test Q ph4s:\",y_test.sum(),'\\n')\n",
    "\n",
    "X_train_clstrs, X_test_clstrs = get_clusters(x_train, x_test, 2)\n",
    "\n",
    "#print(x_train_clstrs)\n",
    "\n",
    "X_train_scaled, X_test_scaled = scale_features(X_train_clstrs, X_test_clstrs)\n",
    "\n",
    "#print(X_train_scaled)\n",
    "\n",
    "# to divide the df by cluster, we need to ensure we use the correct class labels, we'll use pandas to do that\n",
    "train_clusters = X_train_scaled.copy()\n",
    "test_clusters = X_test_scaled.copy()\n",
    "train_clusters['y'] = y_train\n",
    "test_clusters['y'] = y_test\n",
    "\n",
    "#print(y_train)\n",
    "#print(train_clusters)\n",
    "\n",
    "# locate the \"0\" cluster\n",
    "train_0 = train_clusters.loc[train_clusters.clusters < 0] # after scaling, 0 went negtive\n",
    "test_0 = test_clusters.loc[test_clusters.clusters < 0]\n",
    "y_train_0 = train_0.y.values\n",
    "y_test_0 = test_0.y.values\n",
    "# locate the \"1\" cluster\n",
    "train_1 = train_clusters.loc[train_clusters.clusters > 0] # after scaling, 1 dropped slightly\n",
    "test_1 = test_clusters.loc[test_clusters.clusters > 0]\n",
    "y_train_1 = train_1.y.values\n",
    "y_test_1 = test_1.y.values\n",
    "# the base dataset has no \"clusters\" feature\n",
    "X_train_base = X_train_scaled.drop(columns=['clusters'])\n",
    "X_test_base = X_test_scaled.drop(columns=['clusters'])\n",
    "# drop the targets from the training set\n",
    "X_train_0 = train_0.drop(columns=['y'])\n",
    "X_test_0 = test_0.drop(columns=['y'])\n",
    "X_train_1 = train_1.drop(columns=['y'])\n",
    "X_test_1 = test_1.drop(columns=['y'])\n",
    "\n",
    "#print(X_train_0)\n",
    "#print(len(X_test_1))\n",
    "\n",
    "#0 cluster LR model\n",
    "logisticRegr0 = LogisticRegression(max_iter = 5000)\n",
    "logisticRegr0.fit(X_train_0, y_train_0)\n",
    "\n",
    "y_pred = (logisticRegr0.predict(X_test_0))\n",
    "confmat = confusion_matrix(y_test_0, y_pred, labels=[0,1])\n",
    "#print(confmat)\n",
    "\n",
    "confmat = confusion_matrix(y_test_0, y_pred, labels=[0,1]).ravel()\n",
    "FP = (confmat[1])\n",
    "TP = (confmat[3])\n",
    "\n",
    "PPV = (TP / (TP + FP))\n",
    "\n",
    "cm = pd.crosstab(y_test_0, y_pred, rownames=['Actual'], colnames=['Predicted'], margins=False)\n",
    "\n",
    "print('0 cluster model\\n')\n",
    "print(cm,'\\n')\n",
    "\n",
    "print('PPV:', format(PPV, '.2f'),'\\n')\n",
    "\n",
    "#1 cluster LR model\n",
    "logisticRegr1 = LogisticRegression(max_iter = 5000)\n",
    "logisticRegr1.fit(X_train_1, y_train_1)\n",
    "\n",
    "y_pred = (logisticRegr1.predict(X_test_1))\n",
    "confmat = confusion_matrix(y_test_1, y_pred, labels=[0,1])\n",
    "#print(confmat)\n",
    "\n",
    "confmat = confusion_matrix(y_test_1, y_pred, labels=[0,1]).ravel()\n",
    "FP = (confmat[1])\n",
    "TP = (confmat[3])\n",
    "\n",
    "PPV = (TP / (TP + FP))\n",
    "\n",
    "cm = pd.crosstab(y_test_1, y_pred, rownames=['Actual'], colnames=['Predicted'], margins=False)\n",
    "\n",
    "print('1 cluster model\\n')\n",
    "print(cm,'\\n')\n",
    "\n",
    "print('PPV:', format(PPV, '.2f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D2 Q ph4s: 78 \n",
      "\n",
      "0 cluster model\n",
      "\n",
      "Predicted     0     1\n",
      "Actual               \n",
      "0          2403  2181\n",
      "1             1    70 \n",
      "\n",
      "PPV: 0.03 \n",
      "\n",
      "1 cluster model\n",
      "\n",
      "Predicted    0    1\n",
      "Actual             \n",
      "0          115  154\n",
      "1            4    3 \n",
      "\n",
      "PPV: 0.02\n",
      "276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Greg\\anaconda3\\lib\\site-packages\\sklearn\\base.py:488: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names must be in the same order as they were in fit.\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Greg\\anaconda3\\lib\\site-packages\\sklearn\\base.py:488: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names must be in the same order as they were in fit.\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "def scale_features_single(X: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    applies standard scaler (z-scores) to training data and predicts z-scores for the test set\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    to_scale = [col for col in X.columns.values]\n",
    "    scaler.fit(X[to_scale])\n",
    "    X[to_scale] = scaler.transform(X[to_scale])\n",
    "    \n",
    "    return X\n",
    "\n",
    "#CLassify external data (D2 6LUQ pharmacophore models)\n",
    "#Steps for creating \"refined\" external dataset:\n",
    "#1. delete max_feat >15\n",
    "#2. delete min_feat >5\n",
    "#3.\n",
    "ext_df = pd.read_csv('..\\data\\D2_6LUQ_pharmacophores_binary.csv')\n",
    "ext_df.drop(['receptor','Active_Rate','Enrichment', 'GH', 'Actives', 'filename', 'fbase', 'hyd', 'don', 'acc', 'ani', 'cat', 'aro', 'donhyd', 'catdon', 'hydaro', 'aniacc', 'donacc','don_prop', 'acc_prop', 'ani_prop', 'cat_prop', 'aro_prop', 'donhyd_prop', 'hydaro_prop', 'donacc_prop'], 1, inplace=True)\n",
    "ext_df.fillna(-99999)\n",
    "\n",
    "x = ext_df.drop('quality', 1)\n",
    "y = ext_df.quality\n",
    "\n",
    "print(\"D2 Q ph4s:\", y.sum(),'\\n')\n",
    "\n",
    "clustering = KMeans(n_clusters=2, random_state=8675309)\n",
    "clustering.fit(x)\n",
    "\n",
    "train_labels = clustering.labels_\n",
    "\n",
    "X_clstrs = x.copy()\n",
    "X_clstrs['clusters'] = train_labels\n",
    "\n",
    "X_scaled = scale_features_single(X_clstrs)\n",
    "ext_clusters = X_scaled.copy()\n",
    "ext_clusters['y'] = y\n",
    "\n",
    "# locate the \"0\" cluster\n",
    "ext_0 = ext_clusters.loc[ext_clusters.clusters < 0] # after scaling, 0 went negtive\n",
    "y_ext_0 = ext_0.y.values\n",
    "\n",
    "# locate the \"1\" cluster\n",
    "ext_1 = ext_clusters.loc[ext_clusters.clusters > 0] # after scaling, 0 went negtive\n",
    "y_ext_1 = ext_1.y.values\n",
    "\n",
    "# drop the targets from the external set\n",
    "X_ext_0 = ext_0.drop(columns=['y'])\n",
    "X_ext_1 = ext_1.drop(columns=['y'])\n",
    "\n",
    "#predict based on 0 cluster model\n",
    "y_pred = (logisticRegr0.predict(X_ext_0))\n",
    "confmat = confusion_matrix(y_ext_0, y_pred, labels=[0,1])\n",
    "#print(confmat)\n",
    "\n",
    "confmat = confusion_matrix(y_ext_0, y_pred, labels=[0,1]).ravel()\n",
    "FP = (confmat[1])\n",
    "TP = (confmat[3])\n",
    "\n",
    "PPV = (TP / (TP + FP))\n",
    "\n",
    "cm = pd.crosstab(y_ext_0, y_pred, rownames=['Actual'], colnames=['Predicted'], margins=False)\n",
    "\n",
    "print('0 cluster model\\n')\n",
    "print(cm,'\\n')\n",
    "\n",
    "print('PPV:', format(PPV, '.2f'),'\\n')\n",
    "\n",
    "#1 cluster LR model\n",
    "y_pred = (logisticRegr1.predict(X_ext_1))\n",
    "confmat = confusion_matrix(y_ext_1, y_pred, labels=[0,1])\n",
    "#print(confmat)\n",
    "\n",
    "confmat = confusion_matrix(y_ext_1, y_pred, labels=[0,1]).ravel()\n",
    "FP = (confmat[1])\n",
    "TP = (confmat[3])\n",
    "\n",
    "PPV = (TP / (TP + FP))\n",
    "\n",
    "cm = pd.crosstab(y_ext_1, y_pred, rownames=['Actual'], colnames=['Predicted'], margins=False)\n",
    "\n",
    "print('1 cluster model\\n')\n",
    "print(cm,'\\n')\n",
    "\n",
    "print('PPV:', format(PPV, '.2f'))\n",
    "\n",
    "print(len(ext_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
